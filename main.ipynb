{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import glob\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# For data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Pytorch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda import amp\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "\n",
    "# Utils\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# Sklearn Imports\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# For Image Models\n",
    "import timm\n",
    "\n",
    "# Albumentations for augmentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# For colored terminal text\n",
    "from colorama import Fore, Back, Style\n",
    "b_ = Fore.BLUE\n",
    "sr_ = Style.RESET_ALL\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# For descriptive error messages\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "\n",
    "# Auxiliary Functions\n",
    "import auxiliaryfunctions as af\n",
    "import imageprocessing as ip\n",
    "import myclasses as mc\n",
    "import aefiles as ae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"seed\": 42,    # for reproducibility\n",
    "    \"img_size\": 2048,\n",
    "    \"model_name\": \"tf_efficientnetv2_s_in21ft1k\",\n",
    "    \"num_classes\": 5,\n",
    "    \"valid_batch_size\":4,\n",
    "    \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "af.set_seed(CONFIG['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ROOT_DIR = '/kaggle/input/UBC-OCEAN'\n",
    "TEST_DIR = '/kaggle/input/UBC-OCEAN/test_thumbnails'\n",
    "TRAIN_DIR = '/kaggle/input/UBC-OCEAN/train_thumbnails'\n",
    "\n",
    "ALT_TEST_DIR = '/kaggle/input/UBC-OCEAN/test_images'\n",
    "ALT_TRAIN_DIR = '/kaggle/input/UBC-OCEAN/train_images'\n",
    "\n",
    "LABEL_ENCODER_BIN = \"....\"\n",
    "BEST_WEIGHT = \"....\"\n",
    "BEST_WEIGHT2 = \"....\"\n",
    "BEST_WEIGHT3 = \"....\"\n",
    "BEST_WEIGHT4 = \"....\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Reading in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{ROOT_DIR}/test.csv\")\n",
    "df['file_path'] = df['image_id'].apply(af.get_test_file_path)\n",
    "df['label'] = 0 # dummy\n",
    "df_sub = pd.read_csv(f\"{ROOT_DIR}/sample_submission.csv\")\n",
    "encoder = joblib.load( LABEL_ENCODER_BIN )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for (file_path, image_id) in zip(df[\"file_path\"], df[\"image_id\"]):\n",
    "    dfs.append( ip.get_cropped_images(file_path, image_id, None) )\n",
    "\n",
    "df_crop = pd.concat(dfs)\n",
    "df_crop[\"label\"] = 0 # dummy\n",
    "df_crop = df_crop.drop_duplicates(subset=[\"image_id\", \"sx\", \"ex\", \"sy\", \"ey\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    \"valid\": A.Compose([\n",
    "        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n",
    "        A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "        ToTensorV2()], p=1.)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model0 = mc.UBCModel('tf_efficientnetv2_s_in21ft1k', CONFIG['num_classes'])\n",
    "model2 = mc.UBCModel('tf_efficientnet_b0_ns', CONFIG['num_classes'])\n",
    "model3 = mc.UBCModel('tf_efficientnet_b0_ns', CONFIG['num_classes'])\n",
    "model4 = mc.UBCModel('tf_efficientnet_b0_ns', CONFIG['num_classes'])\n",
    "\n",
    "model0.load_state_dict(torch.load( BEST_WEIGHT ))\n",
    "model0.to(CONFIG['device']);\n",
    "model2.load_state_dict(torch.load( BEST_WEIGHT2 ))\n",
    "model2.to(CONFIG['device']);\n",
    "model3.load_state_dict(torch.load( BEST_WEIGHT3 ))\n",
    "model3.to(CONFIG['device']);\n",
    "model4.load_state_dict(torch.load( BEST_WEIGHT3 ))\n",
    "model4.to(CONFIG['device']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exclude_params = [\"model.conv_stem.weight\", \"model.bn1.weight\"]\n",
    "include_params  = [\"linear.weight\", \"linear.bias\"]\n",
    "for n, param in model0.named_parameters():\n",
    "    if n not in include_params:\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        param.requires_grad=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoEncoder Model for Outlier Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for training AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(f\"{ROOT_DIR}/train.csv\")    \n",
    "df_train, df_valid = train_test_split(df_train, test_size=0.2, random_state=42)\n",
    "\n",
    "    \n",
    "df_train['file_path'] = df_train['image_id'].apply(af.get_train_file_path)\n",
    "df_valid['file_path'] = df_valid['image_id'].apply(af.get_train_file_path)\n",
    "    \n",
    "df_train['label'] = encoder.transform(df_train['label'])\n",
    "df_valid['label'] = encoder.transform(df_valid['label'])\n",
    "\n",
    "dfs_train = []\n",
    "for (file_path, image_id,label) in zip(df_train[\"file_path\"], df_train[\"image_id\"], df_train['label']):\n",
    "    dfs_train.append(ip.get_cropped_images(file_path, image_id, label) )\n",
    "    \n",
    "dfs_valid = []\n",
    "for (file_path, image_id,label) in zip(df_valid[\"file_path\"], df_valid[\"image_id\"], df_valid['label']):\n",
    "     dfs_valid.append(ip.get_cropped_images(file_path, image_id, label) )\n",
    "        \n",
    "df_crop_train = pd.concat(dfs_train)\n",
    "df_crop_valid = pd.concat(dfs_valid)\n",
    "    \n",
    "df_crop_train = df_crop_train.drop_duplicates(subset=[\"image_id\", \"sx\", \"ex\", \"sy\", \"ey\",\"label\"]).reset_index(drop=True)\n",
    "df_crop_valid = df_crop_valid.drop_duplicates(subset=[\"image_id\", \"sx\", \"ex\", \"sy\", \"ey\",\"label\"]).reset_index(drop=True)\n",
    "        \n",
    "dataset = mc.UBCDataset(df_crop_train, transforms=data_transforms[\"valid\"])\n",
    "dataset_valid = mc.UBCDataset(df_crop_valid, transforms=data_transforms[\"valid\"])\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=2,\n",
    "                          num_workers=2,shuffle=False,pin_memory=False)\n",
    "valid_loader = DataLoader(dataset_valid, batch_size=2,\n",
    "                          num_workers=2,shuffle=False,pin_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoencoder = ae.Autoencoder()\n",
    "autoencoder = autoencoder.to(CONFIG['device'])\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "ae.train_autoencoder(train_loader,valid_loader, autoencoder, criterion, optimizer, num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up DataLoader for the test set\n",
    "test_dataset = mc.UBCDataset(df_crop, transforms=data_transforms[\"valid\"])\n",
    "# with batch size 1\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, \n",
    "                          num_workers=2, shuffle=False, pin_memory=True)\n",
    "\n",
    "# Evaluate the autoencoder on the test set\n",
    "loaded_model = ae.Autoencoder()\n",
    "criterion = nn.MSELoss()\n",
    "loaded_model.load_state_dict(torch.load('....pth'))\n",
    "loaded_model = loaded_model.to(CONFIG['device'])\n",
    "#load with batchsize 1\n",
    "anom_list,ids,probs_test = ae.evaluate_autoencoder(test_loader, loaded_model,df_crop,1)\n",
    "anom_dict = af.combine_anoms(ids, anom_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_dataset = mc.UBCDataset(df_crop, transforms=data_transforms[\"valid\"])\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG['valid_batch_size'], \n",
    "                          num_workers=2, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "with torch.no_grad():\n",
    "    bar = tqdm(enumerate(test_loader), total=len(test_loader))\n",
    "    for step, data in bar:        \n",
    "        images = data['image'].to(CONFIG[\"device\"], dtype=torch.float)        \n",
    "        \n",
    "        outputs1 = model0(images)\n",
    "        outputs2 = model2(images)\n",
    "        outputs3 = model3(images)\n",
    "        outputs4 = model4(images)\n",
    "        \n",
    "        outputs = 0.66 * (0.34 * outputs4 + 0.7 * outputs2) + 0.322 * (0.4 * outputs1 + 0.6 * outputs3)\n",
    "        outputs = model0.softmax(outputs)\n",
    "        \n",
    "        preds.append( outputs.detach().cpu().numpy() )\n",
    "\n",
    "preds = np.vstack(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(preds.shape[-1]):\n",
    "    df_crop[f\"cat{i}\"] = preds[:, i]\n",
    "    \n",
    "\n",
    "\n",
    "dict_label = {}\n",
    "anomaly_label = {}\n",
    "for image_id, gdf in df_crop.groupby(\"image_id\"):\n",
    "    dict_label[image_id] = np.argmax( gdf[ [f\"cat{i}\" for i in range(preds.shape[-1])] ].values.max(axis=0) )\n",
    "    \n",
    "    \n",
    "preds = np.array( [ dict_label[image_id] for image_id in df[\"image_id\"].values ] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_labels = encoder.inverse_transform( preds )\n",
    "df_sub[\"label\"] = pred_labels\n",
    "#df_sub[\"anomalies\"] = anom_list\n",
    "df_sub['anomalies'] = df_sub['image_id'].map(anom_dict)\n",
    "df_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sub[\"label\"] = df_sub.apply(lambda x: af.update_classes(x[\"label\"], x[\"anomalies\"]), axis=1)\n",
    "df_sub = df_sub.drop(columns=[\"anomalies\"])\n",
    "df_sub.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jennyenv)",
   "language": "python",
   "name": "jennyenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
