{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import glob\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# For data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Pytorch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda import amp\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "\n",
    "# Utils\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# Sklearn Imports\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# For Image Models\n",
    "import timm\n",
    "\n",
    "# Albumentations for augmentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# For colored terminal text\n",
    "from colorama import Fore, Back, Style\n",
    "b_ = Fore.BLUE\n",
    "sr_ = Style.RESET_ALL\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# For descriptive error messages\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Auxiliary Functions\n",
    "import auxiliaryfunctions as af\n",
    "import imageprocessing as ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"seed\": 42,\n",
    "    \"img_size\": 2048,\n",
    "    \"model_name\": \"tf_efficientnetv2_s_in21ft1k\",\n",
    "    \"num_classes\": 5,\n",
    "    \"valid_batch_size\":4,\n",
    "    \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "af.set_seed(CONFIG['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ROOT_DIR = '/kaggle/input/UBC-OCEAN'\n",
    "TEST_DIR = '/kaggle/input/UBC-OCEAN/test_thumbnails'\n",
    "TRAIN_DIR = '/kaggle/input/UBC-OCEAN/train_thumbnails'\n",
    "\n",
    "ALT_TEST_DIR = '/kaggle/input/UBC-OCEAN/test_images'\n",
    "ALT_TRAIN_DIR = '/kaggle/input/UBC-OCEAN/train_images'\n",
    "\n",
    "LABEL_ENCODER_BIN = \"....\"\n",
    "BEST_WEIGHT = \"....\"\n",
    "BEST_WEIGHT2 = \"....\"\n",
    "BEST_WEIGHT3 = \"....\"\n",
    "BEST_WEIGHT4 = \"....\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{ROOT_DIR}/test.csv\")\n",
    "df['file_path'] = df['image_id'].apply(af.get_test_file_path)\n",
    "df['label'] = 0 # dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sub = pd.read_csv(f\"{ROOT_DIR}/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = joblib.load( LABEL_ENCODER_BIN )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for (file_path, image_id) in zip(df[\"file_path\"], df[\"image_id\"]):\n",
    "    dfs.append( ip.get_cropped_images(file_path, image_id, None) )\n",
    "\n",
    "df_crop = pd.concat(dfs)\n",
    "df_crop[\"label\"] = 0 # dummy\n",
    "df_crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_crop = df_crop.drop_duplicates(subset=[\"image_id\", \"sx\", \"ex\", \"sy\", \"ey\"]).reset_index(drop=True)\n",
    "df_crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UBCDataset(Dataset):\n",
    "    def __init__(self, df, transforms=None):\n",
    "        self.df = df\n",
    "        self.file_names = df['file_path'].values\n",
    "        self.labels = df['label'].values\n",
    "        self.transforms = transforms\n",
    "        self.sxs = df[\"sx\"].values\n",
    "        self.exs = df[\"ex\"].values\n",
    "        self.sys = df[\"sy\"].values\n",
    "        self.eys = df[\"ey\"].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.file_names[index]\n",
    "        sx = self.sxs[index]\n",
    "        ex = self.exs[index]\n",
    "        sy = self.sys[index]\n",
    "        ey = self.eys[index]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        img = img[ sy:ey, sx:ex, : ]\n",
    "        \n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)[\"image\"]\n",
    "        try:\n",
    "            return {\n",
    "                'image': img,\n",
    "                'label': torch.tensor(label, dtype=torch.long)\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f'{e=} / {label=} / {label.dtype=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    \"valid\": A.Compose([\n",
    "        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n",
    "        A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "        ToTensorV2()], p=1.)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GeM(nn.Module):\n",
    "    def __init__(self, p=3, eps=1e-6):\n",
    "        super(GeM, self).__init__()\n",
    "        self.p = nn.Parameter(torch.ones(1)*p)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gem(x, p=self.p, eps=self.eps)\n",
    "        \n",
    "    def gem(self, x, p=3, eps=1e-6):\n",
    "        return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \\\n",
    "                '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + \\\n",
    "                ', ' + 'eps=' + str(self.eps) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UBCModel(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, pretrained=False, checkpoint_path=None):\n",
    "        super(UBCModel, self).__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained)\n",
    "\n",
    "        in_features = self.model.classifier.in_features\n",
    "        self.model.classifier = nn.Identity()\n",
    "        self.model.global_pool = nn.Identity()\n",
    "        self.pooling = GeM()\n",
    "        self.linear = nn.Linear(in_features, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.model(images)\n",
    "        pooled_features = self.pooling(features).flatten(1)\n",
    "        output = self.linear(pooled_features)\n",
    "        return output\n",
    "\n",
    "    \n",
    "model0 = UBCModel('tf_efficientnetv2_s_in21ft1k', CONFIG['num_classes'])\n",
    "model2 = UBCModel('tf_efficientnet_b0_ns', CONFIG['num_classes'])\n",
    "model3 = UBCModel('tf_efficientnet_b0_ns', CONFIG['num_classes'])\n",
    "model4 = UBCModel('tf_efficientnet_b0_ns', CONFIG['num_classes'])\n",
    "model0.load_state_dict(torch.load( BEST_WEIGHT ))\n",
    "model0.to(CONFIG['device']);\n",
    "model2.load_state_dict(torch.load( BEST_WEIGHT2 ))\n",
    "model2.to(CONFIG['device']);\n",
    "model3.load_state_dict(torch.load( BEST_WEIGHT3 ))\n",
    "model3.to(CONFIG['device']);\n",
    "model4.load_state_dict(torch.load( BEST_WEIGHT3 ))\n",
    "model4.to(CONFIG['device']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exclude_params = [\"model.conv_stem.weight\", \"model.bn1.weight\"]\n",
    "include_params  = [\"linear.weight\", \"linear.bias\"]\n",
    "for n, param in model0.named_parameters():\n",
    "    if n not in include_params:\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        param.requires_grad=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoEncoder Model for Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 3, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            #nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "def train_autoencoder(train_loader,valid_loader, model, criterion, optimizer, reg_strength=1e-5,num_epochs=10):\n",
    "    model.train()\n",
    "    valid_losses_list = []\n",
    "    training_loss_list = []\n",
    "    for epoch in range(num_epochs):\n",
    "        bar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "        for steps,data in bar:\n",
    "            inputs, _ = data['image'].to(CONFIG['device'],dtype=torch.float32), data['label'].to(CONFIG['device'])\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            #loss = criterion(outputs,inputs)\n",
    "            reconstruction_loss = criterion(outputs, inputs)\n",
    "            \n",
    "            # L2 regularization\n",
    "            reg_loss = 0.0\n",
    "            for param in model.parameters():\n",
    "                reg_loss += torch.norm(param, p=2)\n",
    "\n",
    "            loss = reconstruction_loss + reg_strength * reg_loss            \n",
    "            \n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "        print(f'Training - Epoch [{epoch + 1}/{num_epochs}], Reconstruction Loss: {reconstruction_loss.item()}, Regularization Loss: {reg_strength * reg_loss.item()}')\n",
    "        training_loss_list.append(loss.item())\n",
    "    \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_valid_loss = 0.0\n",
    "        losses_within_epoch = []\n",
    "        with torch.no_grad():\n",
    "            for data in valid_loader:\n",
    "                inputs, _ = data['image'].to(CONFIG['device'], dtype=torch.float32), data['label'].to(CONFIG['device'])\n",
    "                outputs = model(inputs)\n",
    "                valid_loss = criterion(outputs, inputs)\n",
    "                total_valid_loss += valid_loss.item()\n",
    "                losses_within_epoch.append(valid_loss.item())\n",
    "        \n",
    "        average_valid_loss = total_valid_loss / len(valid_loader)\n",
    "        print(f'Validation - Epoch [{epoch + 1}/{num_epochs}], Loss: {average_valid_loss}')\n",
    "        valid_losses_list.append(average_valid_loss)\n",
    "    print(f'Validation losses for each epoch: {valid_losses_list}')\n",
    "    print(f'Training losses for each epoch: {training_loss_list}')\n",
    "\n",
    "    torch.save(model.state_dict(), 'autoencoder.pth')\n",
    "\n",
    "\n",
    "train_auto_model = False\n",
    "if train_auto_model !=False:\n",
    "\n",
    "    df_train = pd.read_csv(f\"{ROOT_DIR}/train.csv\")\n",
    "        \n",
    "    df_train, df_valid = train_test_split(df_train, test_size=0.2, random_state=42)\n",
    "\n",
    "    \n",
    "    df_train['file_path'] = df_train['image_id'].apply(af.get_train_file_path)\n",
    "    df_valid['file_path'] = df_valid['image_id'].apply(af.get_train_file_path)\n",
    "\n",
    "    \n",
    "    df_train['label'] = encoder.transform(df_train['label'])\n",
    "    df_valid['label'] = encoder.transform(df_valid['label'])\n",
    "\n",
    "    dfs_train = []\n",
    "    for (file_path, image_id,label) in zip(df_train[\"file_path\"], df_train[\"image_id\"], df_train['label']):\n",
    "        dfs_train.append(ip.get_cropped_images(file_path, image_id, label) )\n",
    "    \n",
    "    dfs_valid = []\n",
    "    for (file_path, image_id,label) in zip(df_valid[\"file_path\"], df_valid[\"image_id\"], df_valid['label']):\n",
    "        dfs_valid.append(ip.get_cropped_images(file_path, image_id, label) )\n",
    "        \n",
    "    df_crop_train = pd.concat(dfs_train)\n",
    "    df_crop_valid = pd.concat(dfs_valid)\n",
    "    \n",
    "    df_crop_train = df_crop_train.drop_duplicates(subset=[\"image_id\", \"sx\", \"ex\", \"sy\", \"ey\",\"label\"]).reset_index(drop=True)\n",
    "    df_crop_valid = df_crop_valid.drop_duplicates(subset=[\"image_id\", \"sx\", \"ex\", \"sy\", \"ey\",\"label\"]).reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    dataset = UBCDataset(df_crop_train, transforms=data_transforms[\"valid\"])\n",
    "    dataset_valid = UBCDataset(df_crop_valid, transforms=data_transforms[\"valid\"])\n",
    "\n",
    "    train_loader = DataLoader(dataset, batch_size=2,\n",
    "                              num_workers=2,shuffle=False,pin_memory=False)\n",
    "    valid_loader = DataLoader(dataset_valid, batch_size=2,\n",
    "                              num_workers=2,shuffle=False,pin_memory=False)\n",
    "    autoencoder = Autoencoder()\n",
    "    autoencoder = autoencoder.to(CONFIG['device'])\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "\n",
    "    train_autoencoder(train_loader,valid_loader, autoencoder, criterion, optimizer, num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_autoencoder(test_loader, model, origin_df, batchsize,threshold=0.075):\n",
    "    model.eval()\n",
    "    anoms = []\n",
    "    image_ids = []\n",
    "    probs = []\n",
    "    with torch.no_grad():\n",
    "        bar = tqdm(enumerate(test_loader), total = len(test_loader))\n",
    "        for steps,data in bar:\n",
    "            inputs, _ = data['image'].to(CONFIG['device'],dtype=torch.float32), data['label'].to(CONFIG['device'])\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "\n",
    "            batch_start_index = steps * batchsize\n",
    "            batch_end_index = (steps + 1) * batchsize\n",
    "            current_batch_ids = origin_df.iloc[batch_start_index:batch_end_index]['image_id'].tolist()\n",
    "        \n",
    "            # Check if the loss is above the threshold, indicating an anomaly\n",
    "            probs.append(loss.item())\n",
    "            if loss.item() > threshold:\n",
    "                print(\"Anomaly detected!\")\n",
    "                #anoms.append(1)\n",
    "                anoms.extend([1] * len(inputs))\n",
    "            else:\n",
    "                #anoms.append(0)\n",
    "                anoms.extend([0] * len(inputs))\n",
    "        \n",
    "            image_ids.extend(current_batch_ids)\n",
    "\n",
    "    return (anoms, image_ids,probs)\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "# Set up DataLoader for the test set\n",
    "test_dataset = UBCDataset(df_crop, transforms=data_transforms[\"valid\"])\n",
    "# with batch size 1\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, \n",
    "                          num_workers=2, shuffle=False, pin_memory=True)\n",
    "\n",
    "# Evaluate the autoencoder on the test set\n",
    "loaded_model = Autoencoder()\n",
    "criterion = nn.MSELoss()\n",
    "loaded_model.load_state_dict(torch.load('....pth'))\n",
    "loaded_model = loaded_model.to(CONFIG['device'])\n",
    "#load with batchsize 1\n",
    "anom_list,ids,probs_test = evaluate_autoencoder(test_loader, loaded_model,df_crop,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "anom_dict = af.combine_anoms(ids, anom_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_dataset = UBCDataset(df_crop, transforms=data_transforms[\"valid\"])\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG['valid_batch_size'], \n",
    "                          num_workers=2, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "with torch.no_grad():\n",
    "    bar = tqdm(enumerate(test_loader), total=len(test_loader))\n",
    "    for step, data in bar:        \n",
    "        images = data['image'].to(CONFIG[\"device\"], dtype=torch.float)        \n",
    "        \n",
    "        outputs1 = model0(images)\n",
    "        outputs2 = model2(images)\n",
    "        outputs3 = model3(images)\n",
    "        outputs4 = model4(images)\n",
    "        \n",
    "        outputs = 0.66 * (0.34 * outputs4 + 0.7 * outputs2) + 0.322 * (0.4 * outputs1 + 0.6 * outputs3)\n",
    "        outputs = model0.softmax(outputs)\n",
    "        \n",
    "        preds.append( outputs.detach().cpu().numpy() )\n",
    "\n",
    "preds = np.vstack(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(preds.shape[-1]):\n",
    "    df_crop[f\"cat{i}\"] = preds[:, i]\n",
    "    \n",
    "\n",
    "\n",
    "dict_label = {}\n",
    "anomaly_label = {}\n",
    "for image_id, gdf in df_crop.groupby(\"image_id\"):\n",
    "    dict_label[image_id] = np.argmax( gdf[ [f\"cat{i}\" for i in range(preds.shape[-1])] ].values.max(axis=0) )\n",
    "    \n",
    "    \n",
    "preds = np.array( [ dict_label[image_id] for image_id in df[\"image_id\"].values ] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_labels = encoder.inverse_transform( preds )\n",
    "df_sub[\"label\"] = pred_labels\n",
    "#df_sub[\"anomalies\"] = anom_list\n",
    "df_sub['anomalies'] = df_sub['image_id'].map(anom_dict)\n",
    "df_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sub[\"label\"] = df_sub.apply(lambda x: af.update_classes(x[\"label\"], x[\"anomalies\"]), axis=1)\n",
    "df_sub = df_sub.drop(columns=[\"anomalies\"])\n",
    "df_sub.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jennyenv)",
   "language": "python",
   "name": "jennyenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
